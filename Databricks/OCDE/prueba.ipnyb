{"cells":[{"cell_type":"markdown","source":["# Using Auto Loader to simplify ingest as the first step in your ETL\n\n### Overview\n[Auto Loader](https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-gen2.html) is an ingest feature of Databricks that makes it simple to incrementally ingest only new data from Azure Data Lake. In this notebook we will use Auto Loader for a basic ingest use case but there are many features of Auto Loader, like [schema inference and evolution](https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-gen2.html#schema-inference-and-evolution), that make it possible to ingest very complex and dynymically changing data.\n\nThe following example ingests financial data. Estimated Earnings Per Share (EPS) is financial data from analysts predicting what a companyâ€™s quarterly earnings per share will be. The raw data can come from many different sources and from multiple analysts for multiple stocks. In this notebook, the data is simply ingested into the bronze table using Auto Loader.\n\n<img src=\"https://raw.githubusercontent.com/jodb/DatabricksAndAzureMapsWorkshop/master/episode4/azureDbIngestEtlArch.png\">"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6da8fa3-8403-4313-bc72-847eb5dd5952"}}},{"cell_type":"code","source":["# autoloader table and checkpoint paths\nbasepath = \"dbfs:/tmp/incrementalETL/\"\nbronzeTable = basepath + \"bronze/\"\nbronzeCheckpoint = basepath + \"bronze/checkpoint/\"\nbronzeSchema = basepath + \"bronze/schema/\"\nsilverTable = basepath + \"silver/\"\nsilverCheckpoint = basepath + \"silver/checkpoint/\"\nlandingZoneLocation = basepath + \"LandingZone/\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Set up storage paths","showTitle":true,"inputWidgets":{},"nuid":"1a8803a3-297d-4040-93f4-d52ac94b8830"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# dbutils.fs.rm(basepath, recurse = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Remove data from the base path, comment out if you would like to run this","showTitle":true,"inputWidgets":{},"nuid":"3e21880e-83aa-4121-a43d-fc91292f03fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [('3/1/2021','a',1,2.2),\\\n     ('3/1/2021','a',2,2.0),\\\n     ('3/1/2021','b',1,1.3),\\\n     ('3/1/2021','b',2,1.2),\\\n     ('3/1/2021','c',1,3.5),\\\n     ('3/1/2021','c',2,2.6)],\n    ('date','stock_symbol','analyst','estimated_eps'))\n# write one new csv file to the landing zone\ndf.repartition(1).write.mode(\"append\").option(\"header\",\"true\").csv(landingZoneLocation)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code used to build the 1st Estimated Earnings Per Share data set to import into the table","showTitle":true,"inputWidgets":{},"nuid":"b38a553b-2566-4352-bb30-42cf1c89bc53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the code below we use [Auto Loader](https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-gen2.html) to ingest the data into the bronze table and well as [schemaLocation](https://docs.databricks.com/spark/latest/structured-streaming/auto-loader-schema.html#schema-inference) which allows us to infer the schema instead of having to define it ahead of time.\n\nIn the writing of the data, we use [trigger.once](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers) and [checkpointing](https://docs.databricks.com/spark/latest/structured-streaming/production.html?_ga=2.205141207.63998281.1636395861-1297538452.1628713993#enable-checkpointing) both of which make Incremental ETL easy which you can read more about in [this blog](https://databricks.com/blog/2021/08/30/how-incremental-etl-makes-life-simpler-with-data-lakes.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb00889e-a229-4796-a0f6-9694b1a854d9"}}},{"cell_type":"code","source":["# \"cloudFiles\" indicates the use of Auto Loader\n# We are using csv and give a schema location so that the current schema can be saved and infered easily\ndfBronze = spark.readStream.format(\"cloudFiles\") \\\n  .option(\"cloudFiles.format\", \"csv\") \\\n  .option(\"cloudFiles.schemaLocation\", bronzeSchema) \\\n  .load(landingZoneLocation)\n\n# The stream will shut itself off when it is finished based on the trigger once feature\n# The checkpoint location saves the state of the ingest when it is shut off so we know where to pick up next time\ndfBronze.writeStream \\\n  .format(\"delta\") \\\n  .trigger(once=True) \\\n  .option(\"checkpointLocation\", bronzeCheckpoint) \\\n  .start(bronzeTable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Trigger once stream from autoloader to bronze table","showTitle":true,"inputWidgets":{},"nuid":"2d891db9-80e1-4b4c-a253-9b6f2413150c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f60f12cadc0&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f60f12cadc0&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dfBronze = spark.read.format(\"delta\").load(bronzeTable)\ndisplay(dfBronze.orderBy(\"date\", \"stock_symbol\", \"analyst\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cf9e3a8-8762-4662-9466-91e0999404e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["3/1/2021","a","1","2.2",null],["3/1/2021","b","1","1.3",null],["3/1/2021","b","2","1.2",null],["3/1/2021","c","2","2.6",null],["3/1/2021","a","2","2.0",null],["3/1/2021","c","1","3.5",null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"date","type":"\"string\"","metadata":"{}"},{"name":"stock_symbol","type":"\"string\"","metadata":"{}"},{"name":"analyst","type":"\"string\"","metadata":"{}"},{"name":"estimated_eps","type":"\"string\"","metadata":"{}"},{"name":"_rescued_data","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>stock_symbol</th><th>analyst</th><th>estimated_eps</th><th>_rescued_data</th></tr></thead><tbody><tr><td>3/1/2021</td><td>a</td><td>1</td><td>2.2</td><td>null</td></tr><tr><td>3/1/2021</td><td>b</td><td>1</td><td>1.3</td><td>null</td></tr><tr><td>3/1/2021</td><td>b</td><td>2</td><td>1.2</td><td>null</td></tr><tr><td>3/1/2021</td><td>c</td><td>2</td><td>2.6</td><td>null</td></tr><tr><td>3/1/2021</td><td>a</td><td>2</td><td>2.0</td><td>null</td></tr><tr><td>3/1/2021</td><td>c</td><td>1</td><td>3.5</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dfSilver = spark.readStream.format(\"delta\").load(bronzeTable)\ndfSilver = dfSilver.drop(\"_rescued_data\")\n\ndfSilver.writeStream \\\n  .format(\"delta\") \\\n  .trigger(once=True) \\\n  .option(\"checkpointLocation\", silverCheckpoint) \\\n  .start(silverTable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Trigger once stream from bronze to silver table - We can also stream FROM a Delta table!","showTitle":true,"inputWidgets":{},"nuid":"717f638d-4446-4b01-bd74-8be857e24c99"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f60de93c970&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f60de93c970&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dfSilver = spark.read.format(\"delta\").load(silverTable)\ndisplay(dfSilver.orderBy(\"date\", \"stock_symbol\", \"analyst\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4b1ad30-9061-4572-8a46-14678ec62860"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["3/1/2021","a","1","2.2"],["3/1/2021","a","2","2.0"],["3/1/2021","b","1","1.3"],["3/1/2021","b","2","1.2"],["3/1/2021","c","1","3.5"],["3/1/2021","c","2","2.6"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"date","type":"\"string\"","metadata":"{}"},{"name":"stock_symbol","type":"\"string\"","metadata":"{}"},{"name":"analyst","type":"\"string\"","metadata":"{}"},{"name":"estimated_eps","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>stock_symbol</th><th>analyst</th><th>estimated_eps</th></tr></thead><tbody><tr><td>3/1/2021</td><td>a</td><td>1</td><td>2.2</td></tr><tr><td>3/1/2021</td><td>a</td><td>2</td><td>2.0</td></tr><tr><td>3/1/2021</td><td>b</td><td>1</td><td>1.3</td></tr><tr><td>3/1/2021</td><td>b</td><td>2</td><td>1.2</td></tr><tr><td>3/1/2021</td><td>c</td><td>1</td><td>3.5</td></tr><tr><td>3/1/2021</td><td>c</td><td>2</td><td>2.6</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [('3/1/2021','a',2,2.4),\\\n     ('4/1/2021','a',1,2.3),\\\n     ('4/1/2021','a',2,2.1),\\\n     ('4/1/2021','b',1,1.3),\\\n     ('4/1/2021','b',2,1.2),\\\n     ('4/1/2021','c',1,3.5),\\\n     ('4/1/2021','c',2,2.6)],\n    ('date','stock_symbol','analyst','estimated_eps'))\n# write one new csv file to the landing zone\ndf.repartition(1).write.mode(\"append\").option(\"header\",\"true\").csv(landingZoneLocation)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Code used to build the 2nd EPS data set to import into the bronze table","showTitle":true,"inputWidgets":{},"nuid":"28ec7865-8db9-4188-9e79-dea0052b007c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["dfBronze = spark.readStream.format(\"cloudFiles\") \\\n  .option(\"cloudFiles.format\", \"csv\") \\\n  .option(\"cloudFiles.schemaLocation\", bronzeSchema) \\\n  .load(landingZoneLocation)\n\ndfBronze.writeStream \\\n  .format(\"delta\") \\\n  .trigger(once=True) \\\n  .option(\"checkpointLocation\", bronzeCheckpoint) \\\n  .start(bronzeTable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Trigger once stream from autoloader to bronze table - Only new records are added","showTitle":true,"inputWidgets":{},"nuid":"27633278-c63b-44c3-a2f0-cca15d42bfbc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f60de95d070&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f60de95d070&gt;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["dfBronze = spark.read.format(\"delta\").load(bronzeTable)\ndisplay(dfBronze.orderBy(\"date\", \"stock_symbol\", \"analyst\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9c2c6ee-bb08-4720-9ff3-fd6007d2872c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["3/1/2021","a","1","2.2",null],["3/1/2021","a","2","2.4",null],["3/1/2021","a","2","2.0",null],["3/1/2021","b","1","1.3",null],["3/1/2021","b","2","1.2",null],["3/1/2021","c","1","3.5",null],["3/1/2021","c","2","2.6",null],["4/1/2021","a","1","2.3",null],["4/1/2021","a","2","2.1",null],["4/1/2021","b","1","1.3",null],["4/1/2021","b","2","1.2",null],["4/1/2021","c","1","3.5",null],["4/1/2021","c","2","2.6",null]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"date","type":"\"string\"","metadata":"{}"},{"name":"stock_symbol","type":"\"string\"","metadata":"{}"},{"name":"analyst","type":"\"string\"","metadata":"{}"},{"name":"estimated_eps","type":"\"string\"","metadata":"{}"},{"name":"_rescued_data","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>stock_symbol</th><th>analyst</th><th>estimated_eps</th><th>_rescued_data</th></tr></thead><tbody><tr><td>3/1/2021</td><td>a</td><td>1</td><td>2.2</td><td>null</td></tr><tr><td>3/1/2021</td><td>a</td><td>2</td><td>2.4</td><td>null</td></tr><tr><td>3/1/2021</td><td>a</td><td>2</td><td>2.0</td><td>null</td></tr><tr><td>3/1/2021</td><td>b</td><td>1</td><td>1.3</td><td>null</td></tr><tr><td>3/1/2021</td><td>b</td><td>2</td><td>1.2</td><td>null</td></tr><tr><td>3/1/2021</td><td>c</td><td>1</td><td>3.5</td><td>null</td></tr><tr><td>3/1/2021</td><td>c</td><td>2</td><td>2.6</td><td>null</td></tr><tr><td>4/1/2021</td><td>a</td><td>1</td><td>2.3</td><td>null</td></tr><tr><td>4/1/2021</td><td>a</td><td>2</td><td>2.1</td><td>null</td></tr><tr><td>4/1/2021</td><td>b</td><td>1</td><td>1.3</td><td>null</td></tr><tr><td>4/1/2021</td><td>b</td><td>2</td><td>1.2</td><td>null</td></tr><tr><td>4/1/2021</td><td>c</td><td>1</td><td>3.5</td><td>null</td></tr><tr><td>4/1/2021</td><td>c</td><td>2</td><td>2.6</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Auto Loader ETL","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3473608430194884}},"nbformat":4,"nbformat_minor":0}
